{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qiustander/Simple_MCMC_tasks/blob/main/MCMC_For_Multivariate_Normal_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies & Prerequisites\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>.\n",
        "    <br>\n",
        "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
        "    <br>\n",
        "      <ul>\n",
        "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
        "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
        "In summary, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
        "</div>"
      ],
      "metadata": {
        "id": "NUXq38m1f7mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and Global Variables  { display-mode: \"form\" }\n",
        "\"\"\"Import necessary packages and set global variables\n",
        "\"\"\"\n",
        "!pip3 install -q pandas_datareader\n",
        "!pip3 install -q wget\n",
        "!pip3 install statsmodels\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
        "matplotlib_style = 'seaborn' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import matplotlib.axes as axes;\n",
        "from matplotlib.patches import Ellipse\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas_datareader.data as web\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "from IPython.core.pylabtools import figsize\n",
        "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Eager Execution\n",
        "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
        "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
        "use_tf_eager = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "    try:\n",
        "        tf.enable_eager_execution()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "from tensorflow_probability.python.internal import prefer_static as ps\n",
        "\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "\n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear\n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "def credible_interval(posterior_dist, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compute the credible interval for a set of samples.\n",
        "\n",
        "    Parameters:\n",
        "    - posterior_dist: array-like, the set of samples\n",
        "    - alpha: float, desired significance level (e.g., 0.05 for 95% credible interval)\n",
        "\n",
        "    Returns:\n",
        "    - tuple containing the lower and upper bounds of the credible interval\n",
        "    \"\"\"\n",
        "    sorted_samples = np.sort(posterior_dist)\n",
        "    lower_bound = np.percentile(sorted_samples, 100 * alpha / 2)\n",
        "    upper_bound = np.percentile(sorted_samples, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return (lower_bound, upper_bound)\n",
        "\n",
        "\n",
        "from tensorflow_probability.python.mcmc import RandomWalkMetropolis\n",
        "from tensorflow_probability.python.mcmc import HamiltonianMonteCarlo\n",
        "from tensorflow_probability.python.mcmc import sample_chain\n",
        "\n",
        "# Get TensorFlow version.\n",
        "print(f'TnesorFlow version: {tf.__version__}')\n",
        "print(f'TnesorFlow Probability version: {tfp.__version__}')\n"
      ],
      "metadata": {
        "id": "ZsDVen06f22A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCMC For Fitting 3D Normal Distribution\n",
        "\n",
        "We have $N$ points observed data: $p(\\mathbf{x_i}|\\mathbf{\\mu}, \\Sigma)\\in \\mathbb{R}^N \\sim \\mathcal{N}(\\mathbf{\\mu}, \\Sigma), i = 1, \\dots, N$, where $\\mu \\in \\mathbb{R}^N$ or $σ^2 \\in \\mathbb{R}_+^{N \\times N}$ maybe unknown. $\\mathbf{x_i}$ denotes $n$-th data sample.\n",
        "\n",
        "Problem:\n",
        "1. estimate $p(\\mu|\\mathbf{x},\\Sigma) ∝ \\pi(\\mu)l(\\mathbf{x}|\\mu, \\Sigma)$\n",
        "2. estimate $p(\\Sigma|\\mathbf{x},\\mu) ∝ \\pi(\\Sigma)l(\\mathbf{x}|\\mu, \\Sigma)$\n",
        "3. estimate $p(\\Sigma, \\mu|\\mathbf{x}) ∝ \\pi(\\mu)\\pi(\\Sigma)l(\\mathbf{x}|\\mu, \\Sigma)$,\n",
        "\n",
        "where $\\pi(\\cdot)$ is the prior distribution and $l(\\cdot)$ denotes the data likelihood.\n",
        "\n",
        "We generate sample data from a multivariate normal distribution using TensorFlow Probability Distributions."
      ],
      "metadata": {
        "id": "j0qt70BGnqG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "seed = 50\n",
        "tf.random.set_seed(seed=seed)\n",
        "# Number of samples.\n",
        "n = 100\n",
        "signal_dim = 3\n",
        "# True parameters\n",
        "true_mean = tf.constant([1.2, 0.5, 1.8])\n",
        "true_std = tf.constant([[0.8, 0, 0], [0.6, 0.2, 0], [0.1, 0.4, 1.5]]) # define a lower triangular cholesky part\n",
        "\n",
        "# Define Normal distribution with the true parameters, event_shape:1,\n",
        "normal_true = tfd.MultivariateNormalTriL(loc=true_mean,\n",
        "                                          scale_tril=true_std)\n",
        "# Generate samples.\n",
        "observations = normal_true.sample(sample_shape=n)\n",
        "sample_mean = tf.reduce_mean(observations, axis=0)\n",
        "sample_std = tfp.stats.cholesky_covariance(observations, sample_axis=0)\n",
        "\n",
        "print(f'sample mean: {sample_mean}')\n",
        "print(f'sample std: {sample_std}')\n",
        "\n",
        "PRIOR_MEAN = tf.constant([1.0, 1.0, 1.0])\n",
        "PRIOR_STD = tf.constant([[1.0, 0, 0], [1.0, 1.0, 0], [1.0, 1.0, 1.0]])\n",
        "PRIOR_DF = 3\n",
        "PRIOR_SCALE= tf.linalg.diag(tf.random.uniform([signal_dim]))/PRIOR_DF\n"
      ],
      "metadata": {
        "id": "ZPu6kGO8ghTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\mathbf{\\mu}$, given $\\Sigma$\n",
        "\n",
        "First of all, define a prior distribution which represents the belief for $\\mathbf{\\mu}$.\n",
        "\n",
        "Define $\\pi(\\mu) \\sim \\mathcal{N}(\\mathbf{\\mu}_0, \\Sigma_0)$ which is a multivariate normal distribution.\n",
        "\n",
        "Data likelihood function $l(\\mathbf{x}|\\mathbf{\\mu}, \\Sigma) = \\prod_{i=1} \\mathcal{N}(\\mathbf{x}_i| \\mu, \\Sigma)$.\n",
        "\n",
        "Therefore, the posterior distribution $p(\\mathbf{\\mu}|\\Sigma, \\mathbf{x})$ is still a multivariate normal distribution, it is actually the conjugate prior to the prior distribution. In practice, we apply $\\log$  likelihood and prior distribution such that we just need  addition instead of product which may result in underflow floating point precision."
      ],
      "metadata": {
        "id": "8cK5FdfttAut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_mcmc_samples = 6000\n",
        "num_burn_in_steps = int(num_mcmc_samples//3)\n",
        "# The number of chains is determined by the shape of the initial values.\n",
        "# Here we'll generate N_CHAINS chains, so we'll need a tensor of N_CHAINS initial values.\n",
        "N_CHAINS = 8\n",
        "\n",
        "# Replicate the observations for each chain\n",
        "replicate_observations = tf.tile(tf.expand_dims(observations, axis=1), multiples=[1, N_CHAINS, 1])"
      ],
      "metadata": {
        "id": "5CH2T_q17Pfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = tf.random.normal([N_CHAINS, signal_dim])\n",
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(parameter):\n",
        "  return tfd.MultivariateNormalTriL(loc=PRIOR_MEAN,\n",
        "                                  scale_tril=PRIOR_STD).log_prob(parameter)\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(parameter):\n",
        "        return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=parameter,\n",
        "                                                        scale_tril=sample_std).log_prob(observations), axis=0) \\\n",
        "                                                        + prior_dist(parameter)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.1))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=5,\n",
        "                                        kernel=mh_kernel,\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.02,\n",
        "                                    num_leapfrog_steps=2)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=hmc_kernel,\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n"
      ],
      "metadata": {
        "id": "pHrWoW1PmnZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analytical Posterior Distribution\n",
        "\n",
        "Assume that given $\\Sigma$, define the prior of the $\\mu$ as the $\\pi(\\mu) \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$, the posterior distribution of $\\mu$ is also a Normal distribution that has the analytical form:\n",
        "$$\n",
        "\\mathcal{N}\\left( \\left(\\boldsymbol{\\Sigma}_0^{-1}+n \\boldsymbol{\\Sigma}^{-1}\\right)^{-1}\\left(\\boldsymbol{\\Sigma}_0^{-1} \\boldsymbol{\\mu}_0+n \\boldsymbol{\\Sigma}^{-1} \\overline{\\mathbf{x}}\\right), \\left(\\boldsymbol{\\Sigma}_0^{-1}+n \\boldsymbol{\\Sigma}^{-1}\\right)^{-1}\\right)\n",
        "$$ where $\\overline{\\mathbf{x}}$ is the sample mean"
      ],
      "metadata": {
        "id": "PElHoXbn6FBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# very stupid computation for simplicity\n",
        "prior_inv = tf.linalg.triangular_solve(PRIOR_STD, tf.eye(tf.shape(PRIOR_STD)[0], dtype=PRIOR_STD.dtype), lower=True)\n",
        "prior_cov_inv = tf.matmul(prior_inv, prior_inv, transpose_a=True)\n",
        "\n",
        "sample_inv = tf.linalg.triangular_solve(sample_std, tf.eye(tf.shape(sample_std)[0], dtype=sample_std.dtype), lower=True)\n",
        "sample_cov_inv = tf.matmul(sample_inv, sample_inv, transpose_a=True)\n",
        "\n",
        "coeff = tf.linalg.inv(prior_cov_inv + ps.shape(observations)[0]*sample_cov_inv)\n",
        "\n",
        "true_posterior_mean = tf.linalg.matvec(coeff,\n",
        "                                       tf.linalg.matvec(prior_cov_inv, PRIOR_MEAN)\n",
        "                                       + ps.shape(observations)[0]*tf.linalg.matvec(sample_cov_inv, sample_mean))\n",
        "true_posterior_std = tf.linalg.cholesky(coeff)\n",
        "\n",
        "\n",
        "#quick sanity check of matrix multiplication\n",
        "\n",
        "# PRIOR_STD = tf.constant([[np.random.uniform(), 0, 0], [np.random.uniform(), np.random.uniform(), 0], [np.random.uniform(), np.random.uniform(), np.random.uniform()]])\n",
        "# a = tf.matmul(PRIOR_STD, PRIOR_STD, transpose_b=True)\n",
        "# b = tf.linalg.inv(a)\n",
        "# c = tf.linalg.triangular_solve(PRIOR_STD, tf.eye(tf.shape(PRIOR_STD)[0], dtype=PRIOR_STD.dtype), lower=True)\n",
        "# d = tf.matmul(c, c, transpose_a=True)"
      ],
      "metadata": {
        "id": "szvTEFpE_-Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diagnostics for MCMC sampler\n",
        "\n",
        "Tensorflow probability provides various diagnostic tools to check the convergence of the chain."
      ],
      "metadata": {
        "id": "FNbnhDJy0JB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "\n",
        "accept_rate_mh = tf.math.count_nonzero(mh_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")\n",
        "\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states,\n",
        "    cross_chain_dims=1,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states,\n",
        "    cross_chain_dims=1,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")"
      ],
      "metadata": {
        "id": "4tw5DweZyoRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first chain for plotting auto-correlation,\n",
        "inspect_mean = 1\n",
        "\n",
        "sm.graphics.tsa.plot_acf(mh_states[:, 0, inspect_mean],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states[:, 0, inspect_mean],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states[:, 0, inspect_mean]\n",
        "hmc_dist = hmc_states[:, 0, inspect_mean]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     mh_dist,\n",
        "#     np.abs(np.random.randn(mh_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean[inspect_mean], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean[inspect_mean], color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     hmc_dist,\n",
        "#     np.abs(np.random.randn(hmc_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean[inspect_mean], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean[inspect_mean], color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0k-PKHmMAmmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trace the fist chain\n",
        "plt.plot(tf.squeeze(mh_states[-num_mcmc_samples//5:, 0]))\n",
        "plt.title(\"Trace Plot of MH algorithm\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(tf.squeeze(hmc_states[-num_mcmc_samples//5:, 0]))\n",
        "plt.title(\"Trace Plot of HMC algorithm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7RgifJCLD7ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\Sigma$, given $\\mathbf{\\mu}$\n",
        "\n",
        "First of all, define a prior distribution which represents the belief for $\\Sigma$.\n",
        "\n",
        "Define $\\pi(\\Sigma) \\sim \\mathcal{IW}(\\mathbf{\\nu}_0, \\Psi_0)$ which is a Inverse-Wishart distribution. However, TFP does not have Inverse-Wishart distribution. We can transfer the problem to estimate the precsion matrix $\\pi(\\Lambda) \\sim \\mathcal{W}(\\mathbf{\\nu}_0, V_0)$ which is the inverse matrix of the covariance matrix. The prior distribution is the Wishart distribution.\n",
        "\n",
        "Data likelihood function $l(\\mathbf{x}|\\mathbf{\\mu}, \\Lambda) = \\prod_{i=1} \\mathcal{N}(\\mathbf{x}_i| \\mu, \\Lambda)$.\n",
        "\n",
        "Therefore, the posterior distribution $p(\\Lambda|\\mathbf{\\mu}, \\mathbf{x})$ is still a Wishart distribution, it is actually the conjugate prior to the prior distribution. In practice, we apply $\\log$  likelihood and prior distribution such that we just need  addition instead of product which may result in underflow floating point precision."
      ],
      "metadata": {
        "id": "ZhICyM0C-ADS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampling In Unconstrainted Space\n",
        "\n",
        "Similar with the univariate case, we would apply *TransformedTransitionKernel* to sample in the unconstrainted space.\n",
        "\n",
        "In multivariate case, the transform woule be a bit complicated. We need 3 steps to transform the unconstrainted parameters back to the constrainted precision matrix - lower triangular, and the diagonal elements must be positive.\n",
        "\n",
        "1. Transform the vector to the lower triangular matrix. In the unconstrainted space, we can sample the lower triangular parts of the precision matrix which is a vector with $\\frac{N(N+1)}{2}$ elements.\n",
        "\n",
        "2. Exponentiate the diagonal elements to ensure the positive values\n",
        "\n",
        "3. Compute $g(X) = XX^T$ where $X$ is lower-triangular, positive-diagonal matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRqYu1ftWvaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = tf.linalg.diag(tf.random.uniform([N_CHAINS, signal_dim]))\n",
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(parameter):\n",
        "\n",
        "  return tfd.WishartTriL(\n",
        "      df=PRIOR_DF,\n",
        "      scale_tril=tf.linalg.cholesky(PRIOR_SCALE)).log_prob(parameter)\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(parameter):\n",
        "      precisions_cholesky = tf.linalg.cholesky(parameter)\n",
        "      covariances = tf.linalg.cholesky_solve(\n",
        "      precisions_cholesky, tf.linalg.eye(signal_dim, batch_shape=[N_CHAINS]))\n",
        "      return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=sample_mean,\n",
        "                                                        scale_tril=tf.linalg.cholesky(covariances)).log_prob(observations), axis=0) \\\n",
        "                                                        + prior_dist(parameter)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "transformed_bijector = tfb.Chain([\n",
        "    # step 3: take the product of Cholesky factors\n",
        "    tfb.CholeskyOuterProduct(),\n",
        "    # step 2: exponentiate the diagonals\n",
        "    tfb.TransformDiagonal(tfb.Exp()),\n",
        "    # step 1: map a vector to a lower triangular matrix\n",
        "    tfb.FillTriangular(),\n",
        "])\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.1))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start, we use sigma as matrix here\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=4,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(mh_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.03,\n",
        "                                    num_leapfrog_steps=3)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(hmc_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n"
      ],
      "metadata": {
        "id": "KdKA2VaNUVcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analytical Posterior Distribution\n",
        "\n",
        "Assume that given $\\mu$, define the prior of the $\\Lambda$ as the $\\pi(\\Lambda) \\sim \\mathcal{W}(\\mathbf{\\nu}_0, V_0)$, the posterior distribution of $\\Lambda$ is also a Wishart distribution that has the analytical form:\n",
        "$\\mathcal{W}\\left(n+\\nu_0,\\left(\\mathbf{V}_0^{-1}+\\sum_{i=1}^n\\left(\\mathbf{x}_{\\mathbf{i}}-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{x}_{\\mathbf{i}}-\\boldsymbol{\\mu}\\right)^T\\right)^{-1}\\right)$"
      ],
      "metadata": {
        "id": "WWQ434ZdCS2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics  { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "accept_rate_mh = tf.math.count_nonzero(mh_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OZI3Be8heM24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_a = int(PRIOR_DF + ps.shape(observations)[0])\n",
        "posterior_b = tf.linalg.inv(tf.linalg.inv(PRIOR_SCALE) + tf.matmul(observations - sample_mean, observations - sample_mean, transpose_a=True))\n",
        "poster_dist  = tfd.WishartTriL(\n",
        "      df=posterior_a,\n",
        "      scale_tril=tf.linalg.cholesky(posterior_b))\n",
        "true_posterior_mean = poster_dist.mean()\n",
        "true_posterior_std = poster_dist.stddev()\n",
        "\n",
        "precisions_cholesky = tf.linalg.cholesky(true_posterior_mean)\n",
        "covariances = tf.linalg.cholesky_solve(\n",
        "precisions_cholesky, tf.linalg.eye(signal_dim))\n",
        "true_posterior_std = tf.linalg.cholesky(covariances)\n",
        "mh_states = tf.linalg.cholesky(tf.linalg.inv(mh_states))\n",
        "hmc_states = tf.linalg.cholesky(tf.linalg.inv(hmc_states))"
      ],
      "metadata": {
        "id": "HyH-mY5UCyU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# batch, num_chain, signal_dim, signal_dim\n",
        "\n",
        "# use the first chain for plotting auto-correlation,\n",
        "inspect_cov = 0\n",
        "\n",
        "sm.graphics.tsa.plot_acf(mh_states[:, 0, inspect_cov, inspect_cov],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states[:, 0, inspect_cov, inspect_cov],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states[:, 0, inspect_cov, inspect_cov]\n",
        "hmc_dist = hmc_states[:, 0, inspect_cov, inspect_cov]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     mh_dist,\n",
        "#     np.abs(np.random.randn(mh_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std[inspect_cov, inspect_cov], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_std[inspect_cov, inspect_cov], color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     hmc_dist,\n",
        "#     np.abs(np.random.randn(hmc_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std[inspect_cov, inspect_cov], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_std[inspect_cov, inspect_cov], color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aXbFVlupkFTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\Sigma$ and $\\mu$\n",
        "\n",
        "Now the problem comes to estimate the joint distribution of $\\Sigma$ and $\\mu$. We transfer the problem to estimate the precision matrix $Λ$ and $μ$, as stated above.\n",
        "\n",
        "We define the prior distribution of $Λ$ as the Wishart distribution.  That is, $\\pi(\\Lambda) \\sim \\mathcal{W}(\\mathbf{\\nu}_0, \\Psi_0)$. And define $\\pi(\\mu) \\sim \\mathcal{N}(\\mu_0, \\Sigma_0^2)$.\n",
        "\n",
        "Data likelihood function $l(x|\\mu, \\Lambda) = \\prod_{i=1} \\mathcal{N}(x_i| \\mu, \\Lambda^{-1})$.\n",
        "\n",
        "The posterior distribution $p(\\Lambda, \\mu|x) ∝ \\pi(\\Lambda) \\pi(\\mu) l(x|\\mu, \\Lambda)$ is a Normal-Wishart distribution."
      ],
      "metadata": {
        "id": "elFeW8dilIJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_state_precision = tf.linalg.diag(tf.random.uniform([N_CHAINS, signal_dim]))\n",
        "init_state_mean = tf.random.normal([N_CHAINS, signal_dim])\n",
        "\n",
        "\"\"\"\n",
        "In this case, the mean and lambda are estimated separately, they are stored in two lists\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(Mu, Lambda):\n",
        "\n",
        "  return tfd.JointDistributionNamed(dict(\n",
        "                Mu=tfd.MultivariateNormalTriL(loc=PRIOR_MEAN,\n",
        "                                  scale_tril=PRIOR_STD),\n",
        "                Lambda=tfd.WishartTriL(\n",
        "                              df=PRIOR_DF,\n",
        "                              scale_tril=tf.linalg.cholesky(PRIOR_SCALE)))).log_prob({'Mu': Mu,\n",
        "                                                                                  'Lambda': Lambda})\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(Mu, Lambda):\n",
        "      precisions_cholesky = tf.linalg.cholesky(Lambda)\n",
        "      covariances = tf.linalg.cholesky_solve(\n",
        "      precisions_cholesky, tf.linalg.eye(signal_dim, batch_shape=[N_CHAINS]))\n",
        "\n",
        "      return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=Mu,\n",
        "                                                        scale_tril=tf.linalg.cholesky(covariances)).log_prob(observations), axis=0) \\\n",
        "                                                        + prior_dist(Mu, Lambda)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "\n",
        "transformed_bijector = tfb.Chain([\n",
        "    # step 3: take the product of Cholesky factors\n",
        "    tfb.CholeskyOuterProduct(),\n",
        "    # step 2: exponentiate the diagonals\n",
        "    tfb.TransformDiagonal(tfb.Exp()),\n",
        "    # step 1: map a vector to a lower triangular matrix\n",
        "    tfb.FillTriangular(),\n",
        "])\n",
        "\n",
        "# We dont need to transform the mean into unconstrainted space thus we just use identity map\n",
        "unconstrained_to_precision = tfb.JointMap(\n",
        "    bijectors=[tfb.Identity(), transformed_bijector]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.1))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=[init_state_mean, init_state_precision], #constant start, we use sigma as matrix here\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=5,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(mh_kernel,\n",
        "                                                                                    bijector=unconstrained_to_precision),\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.03,\n",
        "                                    num_leapfrog_steps=3)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=[init_state_mean, init_state_precision],\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(hmc_kernel,\n",
        "                                                                                    bijector=unconstrained_to_precision),\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n",
        "mh_states_mean = mh_states[0]\n",
        "mh_states_precision = mh_states[1]\n",
        "hmc_states_mean = hmc_states[0]\n",
        "hmc_states_precision = hmc_states[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "XFfYfgcB4ooc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics For Mean { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states_mean,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states_mean,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "accept_rate_mh = tf.math.count_nonzero(mh_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states_mean,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states_mean,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JasZq8y-Kgmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics For Covariance { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states_precision,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states_precision,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states_precision,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states_precision,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-wJ9k_087stM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Of Mean { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "# use the first chain for plotting auto-correlation,\n",
        "inspect_mean = 1\n",
        "\n",
        "sm.graphics.tsa.plot_acf(mh_states_mean[:, 0, inspect_mean],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states_mean[:, 0, inspect_mean],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states_mean[:, 0, inspect_mean]\n",
        "hmc_dist = hmc_states_mean[:, 0, inspect_mean]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     mh_dist,\n",
        "#     np.abs(np.random.randn(mh_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean[inspect_mean], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     hmc_dist,\n",
        "#     np.abs(np.random.randn(hmc_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean[inspect_mean], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcV03xNx7zvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Of Covariance { display-mode: \"form\" }\n",
        "\n",
        "mh_states = tf.linalg.cholesky(tf.linalg.inv(mh_states_precision))\n",
        "hmc_states = tf.linalg.cholesky(tf.linalg.inv(hmc_states_precision))\n",
        "# batch, num_chain, signal_dim, signal_dim\n",
        "\n",
        "# use the first chain for plotting auto-correlation,\n",
        "inspect_cov = 0\n",
        "\n",
        "sm.graphics.tsa.plot_acf(mh_states[:, 0, inspect_cov, inspect_cov],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states[:, 0, inspect_cov, inspect_cov],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states[:, 0, inspect_cov, inspect_cov]\n",
        "hmc_dist = hmc_states[:, 0, inspect_cov, inspect_cov]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     mh_dist,\n",
        "#     np.abs(np.random.randn(mh_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std[inspect_cov, inspect_cov], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     hmc_dist,\n",
        "#     np.abs(np.random.randn(hmc_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std[inspect_cov, inspect_cov], color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7u2TpoWg9D4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}