{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qiustander/Simple_MCMC_tasks/blob/main/MCMC_For_Normal_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies & Prerequisites\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    Tensorflow Probability is part of the colab default runtime, <b>so you don't need to install Tensorflow or Tensorflow Probability if you're running this in the colab</b>.\n",
        "    <br>\n",
        "    If you're running this notebook in Jupyter on your own machine (and you have already installed Tensorflow), you can use the following\n",
        "    <br>\n",
        "      <ul>\n",
        "    <li> For the most recent stable TFP release: <code>pip3 install -q --upgrade tensorflow-probability</code></li>\n",
        "    <li> For the most recent stable GPU-connected version of TFP: <code>pip3 install -q --upgrade tensorflow-probability-gpu</code></li>\n",
        "In summary, if you are running this in a Colab, Tensorflow and TFP are already installed\n",
        "</div>"
      ],
      "metadata": {
        "id": "NUXq38m1f7mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### References:\n",
        "[1] Bayesian Data Analysis 3rd\n",
        "\n",
        "[2] [Several MCMC convergence tools](https://colab.research.google.com/github/bebi103b/bebi103b.github.io/blob/master/lessons/16/mcmc_diagnostics.ipynb#scrollTo=0chfn87t4plw)\n",
        "\n",
        "[3] [TFP MCMC case study](https://www.tensorflow.org/probability/examples/TensorFlow_Probability_Case_Study_Covariance_Estimation#choose_some_parameter_values)"
      ],
      "metadata": {
        "id": "8cOBZ01A3XKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and Global Variables  { display-mode: \"form\" }\n",
        "\"\"\"Import necessary packages and set global variables\n",
        "\"\"\"\n",
        "!pip3 install -q pandas_datareader\n",
        "!pip3 install -q wget\n",
        "!pip3 install statsmodels\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
        "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
        "import matplotlib.axes as axes;\n",
        "from matplotlib.patches import Ellipse\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas_datareader.data as web\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "from IPython.core.pylabtools import figsize\n",
        "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
        "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
        "%config InlineBackend.figure_format = notebook_screen_res\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Eager Execution\n",
        "#@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
        "#@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
        "use_tf_eager = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Use try/except so we can easily re-execute the whole notebook.\n",
        "if use_tf_eager:\n",
        "    try:\n",
        "        tf.enable_eager_execution()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "\n",
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "\n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear\n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "def credible_interval(posterior_dist, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compute the credible interval for a set of samples.\n",
        "\n",
        "    Parameters:\n",
        "    - posterior_dist: array-like, the set of samples\n",
        "    - alpha: float, desired significance level (e.g., 0.05 for 95% credible interval)\n",
        "\n",
        "    Returns:\n",
        "    - tuple containing the lower and upper bounds of the credible interval\n",
        "    \"\"\"\n",
        "    sorted_samples = np.sort(posterior_dist)\n",
        "    lower_bound = np.percentile(sorted_samples, 100 * alpha / 2)\n",
        "    upper_bound = np.percentile(sorted_samples, 100 * (1 - alpha / 2))\n",
        "\n",
        "    return (lower_bound, upper_bound)\n",
        "\n",
        "\n",
        "from tensorflow_probability.python.mcmc import RandomWalkMetropolis\n",
        "from tensorflow_probability.python.mcmc import HamiltonianMonteCarlo\n",
        "from tensorflow_probability.python.mcmc import sample_chain\n",
        "\n",
        "# Get TensorFlow version.\n",
        "print(f'TnesorFlow version: {tf.__version__}')\n",
        "print(f'TnesorFlow Probability version: {tfp.__version__}')\n"
      ],
      "metadata": {
        "id": "ZsDVen06f22A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCMC For Fitting 1D Normal Distribution\n",
        "\n",
        "We have $N$ points observed data: $p(x_i|\\mu, σ^2) \\sim \\mathcal{N}(\\mu, σ^2), i = 1, \\dots, N$, where $\\mu \\in \\mathbb{R}$ or $σ^2 \\in \\mathbb{R}_+$ maybe unknown. $x_i$ denotes $n$-th data sample.\n",
        "\n",
        "Problem:\n",
        "1. estimate $p(\\mu|x,\\sigma^2) ∝ \\pi(\\mu)l(x|\\mu, \\sigma^2)$\n",
        "2. estimate $p(\\sigma^2|x,\\mu) ∝ \\pi(\\sigma^2)l(x|\\mu, \\sigma^2)$\n",
        "3. estimate $p(\\sigma^2, \\mu|x) ∝ \\pi(\\mu)\\pi(\\sigma^2)l(x|\\mu, \\sigma^2)$,\n",
        "\n",
        "where $\\pi(\\cdot)$ is the prior distribution and $l(\\cdot)$ denotes the data likelihood.\n",
        "\n",
        "We generate sample data from a normal distribution using TensorFlow Probability Distributions."
      ],
      "metadata": {
        "id": "j0qt70BGnqG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "seed = 50\n",
        "tf.random.set_seed(seed=seed)\n",
        "# Number of samples.\n",
        "n = 100\n",
        "# True parameters\n",
        "true_mean = tf.constant([1.2])\n",
        "true_std = tf.constant([[0.5]])\n",
        "\n",
        "# Define Normal distribution with the true parameters, event_shape:1,\n",
        "normal_true = tfd.MultivariateNormalTriL(loc=true_mean,\n",
        "                                          scale_tril=true_std)\n",
        "# Generate samples.\n",
        "observations = normal_true.sample(sample_shape=n)\n",
        "sample_mean = tf.reduce_mean(observations)\n",
        "sample_std = tf.math.reduce_std(observations)\n",
        "\n",
        "print(f'sample mean: {sample_mean}')\n",
        "print(f'sample std: {sample_std}')\n",
        "\n",
        "PRIOR_MEAN = tf.constant([1.0])\n",
        "PRIOR_STD = tf.constant([[0.8]])\n",
        "PRIOR_INV_GAMMA_ALPHA = tf.constant([2.0])\n",
        "PRIOR_INV_GAMMA_BETA= tf.constant([2.0])\n"
      ],
      "metadata": {
        "id": "ZPu6kGO8ghTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\mu$, given $\\sigma^2$\n",
        "\n",
        "First of all, define a prior distribution which represents the belief for $\\mu$.\n",
        "\n",
        "Define $\\pi(\\mu) \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, where $\\sigma_0^2 =\\sigma^2$ since it is known, and let $\\mu_0 = \\hat{\\mu}$ which is the sample mean.\n",
        "\n",
        "Data likelihood function $l(x|\\mu, \\sigma^2) = \\prod_{i=1} \\mathcal{N}(x_i| \\mu, \\sigma^2)$.\n",
        "\n",
        "Therefore, the posterior distribution $p(\\mu|\\sigma^2, x) ∝ l(x|\\mu, \\sigma^2)\\pi(\\mu)$ is still a Gaussian distribution, it is actually the conjugate prior to the prior distribution. In practice, we apply $\\log$  likelihood and prior distribution such that we just need  addition instead of product which may result in underflow floating point precision."
      ],
      "metadata": {
        "id": "8cK5FdfttAut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_mcmc_samples = 2000\n",
        "num_burn_in_steps = int(num_mcmc_samples//3)\n",
        "# The number of chains is determined by the shape of the initial values.\n",
        "# Here we'll generate N_CHAINS chains, so we'll need a tensor of N_CHAINS initial values.\n",
        "N_CHAINS = 5\n",
        "\n",
        "# Replicate the observations for each chain\n",
        "replicate_observations = tf.tile(tf.expand_dims(observations, axis=1), multiples=[1, N_CHAINS, 1])"
      ],
      "metadata": {
        "id": "5CH2T_q17Pfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(parameter):\n",
        "  return tfd.Normal(loc=PRIOR_MEAN,\n",
        "                                  scale=PRIOR_STD).log_prob(parameter)\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(parameter):\n",
        "        return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=parameter,\n",
        "                                                        scale_tril=sample_std[tf.newaxis, tf.newaxis]).log_prob(observations), axis=0)[..., tf.newaxis] \\\n",
        "                                                        + prior_dist(parameter)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "init_state = tf.random.normal([N_CHAINS, 1])\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.2))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=0,\n",
        "                                        kernel=mh_kernel,\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.05,\n",
        "                                    num_leapfrog_steps=2)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=hmc_kernel,\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n",
        "mh_states = tf.squeeze(mh_states, axis=[-1]).numpy()\n",
        "hmc_states = tf.squeeze(hmc_states, axis=[-1]).numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "pHrWoW1PmnZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analytical Posterior Distribution\n",
        "\n",
        "Assume that given $\\sigma^2$, define the prior of the $\\mu$ as the $\\pi(\\mu) \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, the posterior distribution of $\\mu$ is also a Normal distribution that has the analytical form:\n",
        "$\\mathcal{N}\\left(\\frac{1}{\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}}\\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{\\sum_{i=1}^n x_i}{\\sigma^2}\\right),\\left(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2}\\right)^{-1}\\right)$"
      ],
      "metadata": {
        "id": "PElHoXbn6FBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coeff = 1/PRIOR_STD**2 + observations.shape[0]/sample_std**2\n",
        "\n",
        "true_posterior_mean = 1/coeff*(tf.reduce_sum(observations)/sample_std**2 + PRIOR_MEAN/PRIOR_STD**2)\n",
        "true_posterior_std = tf.sqrt(1/coeff)"
      ],
      "metadata": {
        "id": "szvTEFpE_-Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diagnostics for MCMC sampler\n",
        "\n",
        "Tensorflow probability provides various diagnostic tools to check the convergence of the chain."
      ],
      "metadata": {
        "id": "FNbnhDJy0JB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Gelman-Rubin R-hat statistic\n",
        "\n",
        "The **Gelman-Rubin R-hat statistic** is a useful metric to determine if we have achieved stationarity with our chains. The idea is that we run multiple chains in parallel (at least four). For a given parameter, we then compute the variance in the samples *between* the chains, and then the variance of samples *within* the chains. The ratio of these two is the Gelman-Rubin R-hat statistic, usually denoted as $\\hat{R}$, and we compute $\\hat{R}$ for each chain.\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{R} = \\frac{\\text{variance between chains}}{\\text{variance within chains}}.\n",
        "\\end{align}\n",
        "\n",
        "The value of $\\hat{R}$ approaches unity if the chains are properly sampling the target distribution because the chains should be identical in their sampling of the posterior if they have all reached the limiting distribution. As a rule of thumb, recommended by [Vehtari, et al.](https://arxiv.org/abs/1903.08008), the value of $\\hat{R}$ should be less than 1.01, but of course this is problem-dependent.\n",
        "\n",
        "The R-hat statistic can be found in *tfp.mcmc.potential_scale_reduction*."
      ],
      "metadata": {
        "id": "Ncr5wP-bH_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")"
      ],
      "metadata": {
        "id": "4tw5DweZyoRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trace Plot\n",
        "\n",
        "One intuitive and easily implemented diagnostic tool is a traceplot which plots the parameter value per iteration against the iteration number. If the model has converged, the traceplot will move around the *mode* of the distribution. A clear sign of non-convergence with a traceplot occurs when we observe some trending in the sample space.\n",
        "\n",
        "**Drawback**: it may appear that the chain has converged, however, it trapped (for a finite time) in a local region rather exploring the full posterior."
      ],
      "metadata": {
        "id": "er032jUOBwU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trace the fist chain\n",
        "plt.plot(tf.squeeze(mh_states[-num_mcmc_samples//5:, 0]))\n",
        "plt.title(\"Trace Plot of MH algorithm\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(tf.squeeze(hmc_states[-num_mcmc_samples//5:, 0]))\n",
        "plt.title(\"Trace Plot of HMC algorithm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7RgifJCLD7ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Acceptance rate"
      ],
      "metadata": {
        "id": "DFJRmiSB8nwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accept_rate_mh = tf.math.count_nonzero(mh_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")"
      ],
      "metadata": {
        "id": "-Q3x6J198n6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Effective samples size\n",
        "\n",
        "Recall that MCMC samplers do not draw independent samples from the target distribution. Rather, the samples are correlated. Ideally, though, we *would* draw independent samples. We would like to get an estimate for the number of *effectively independent* samples we draw. This is referred to either as **effective samples size** (ESS) or number of effective samples ($n_\\mathrm{eff}$).\n",
        "\n",
        "The effective sample size can be found in *tfp.mcmc.effective_sample_size*."
      ],
      "metadata": {
        "id": "Tdmon_LV6XIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")"
      ],
      "metadata": {
        "id": "f6ypXpij9O6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autocorrelation Plot\n",
        "\n",
        "The lag-k autocorrelation is the correlation between every sample and the sample k steps before. This autocorrelation should become smaller as k increases, i.e. samples can be considered as independent. If, on the other hand, autocorrelation remains high for higher values of k , this indicates a high degree of correlation between samples and slow mixing. If the ACF takes too long to decay to 0, the chain exhibits a high degree of dependence and will tend to get stuck in place.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bi5zCidrAmWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first chain for plotting auto-correlation\n",
        "sm.graphics.tsa.plot_acf(mh_states[:, 0],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states[:, 0],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)"
      ],
      "metadata": {
        "id": "0k-PKHmMAmmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Posterior Histogram and Credible Interval\n",
        "\n",
        "It is intuitive to plot the histogram of the posterior distribution to check the convergence. Besides, we can check the credible interval of the samples"
      ],
      "metadata": {
        "id": "Y3Ufw5LS9PHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states[:, 0]\n",
        "hmc_dist = hmc_states[:, 0]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     mh_dist,\n",
        "#     np.abs(np.random.randn(mh_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean, color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "# ax.scatter(\n",
        "#     hmc_dist,\n",
        "#     np.abs(np.random.randn(hmc_dist.size)),\n",
        "#     zorder=15,\n",
        "#     color=\"red\",\n",
        "#     marker=\"x\",\n",
        "#     alpha=0.5,\n",
        "#     label=\"Samples\",\n",
        "# )\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean, color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yWXRCMxmBwAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\sigma^2$, given $\\mu$\n",
        "\n",
        "First of all, define a prior distribution which represents the belief for $\\sigma^2$.\n",
        "\n",
        "We can define the prior distribution of $\\sigma^2$ as the inverse gamma distribution according to the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior). That is, $\\pi(\\sigma^2) \\sim \\mathcal{IG}(\\alpha, \\beta)$.\n",
        "\n",
        "Data likelihood function $l(x|\\mu, \\sigma^2) = \\prod_{i=1} \\mathcal{N}(x_i| \\mu, \\sigma^2)$.\n",
        "\n",
        "Therefore, the posterior distribution $p(\\sigma^2|\\mu, x)$ is a inverse-Gamma distribution, it is actually the conjugate prior to the prior distribution."
      ],
      "metadata": {
        "id": "ZhICyM0C-ADS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sampling In Unconstrainted Space\n",
        "\n",
        "If we directly define the prior distribution and likelihood distribution, the MCMC sampling would definitely fails. The problem here is that our parameter of interest $\\sigma^2$ is a **postive value**. The sampler doesn't know anything about this constraint (except possibly through gradients), so it is entirely possible that the sampler will propose an invalid value, particularly if the step size is large.\n",
        "\n",
        "There is a straightforward solution to the problem above: we can reparameterize our model such that the new parameters no longer have these constraints. TFP provides a useful set of tools - [bijectors](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector) - for doing just that.\n",
        "\n",
        "We can deal with the $\\sigma^2$ with a simple step to transform it to the unconstrainted space: take the log of the $\\sigma^2$. We accomplish this via the inverse of the Exp bijector. Therefore, we can sample the $\\theta = \\log \\sigma^2$, then transform back to original value via $exp(\\theta)$ which is always positive.\n",
        "\n",
        "A even simplier idea to achieve the goal is via the [TransformedTransitionKernel](https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc/TransformedTransitionKernel) which applies a bijector to the MCMC's state space. It wraps the sampler and handles all the conversions. Specifically, with **TransformedTransitionKernel**, the MCMC would automatically maps the $\\theta$ to unconstrainted space, do the sampling, and then transforms back to the original space."
      ],
      "metadata": {
        "id": "eRqYu1ftWvaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = tf.random.uniform([N_CHAINS, 1, 1])\n",
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(parameter):\n",
        "  return tfd.InverseGamma(concentration=PRIOR_INV_GAMMA_ALPHA,\n",
        "                                  scale=tf.constant(PRIOR_INV_GAMMA_BETA)).log_prob(parameter)\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(parameter):\n",
        "        return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=sample_mean,\n",
        "                                                        scale_tril=parameter).log_prob(observations), axis=0)[..., tf.newaxis, tf.newaxis] \\\n",
        "                                                        + prior_dist(parameter)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "\n",
        "transformed_bijector = tfb.Exp()\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.2))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start, we use sigma as matrix here\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=0,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(mh_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.03,\n",
        "                                    num_leapfrog_steps=3)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, #constant start\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(hmc_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n",
        "mh_states = tf.squeeze(mh_states, axis=[-2, -1]).numpy()\n",
        "hmc_states = tf.squeeze(hmc_states, axis=[-2, -1]).numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "KdKA2VaNUVcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analytical Posterior Distribution\n",
        "\n",
        "Assume that given $\\mu$, define the prior of the $\\sigma^2$ as the $\\pi(\\sigma^2) \\sim \\mathcal{IG}(\\alpha, \\beta)$, the posterior distribution of $\\sigma^2$ is also a inverse Gamma distribution that has the analytical form:\n",
        "$\\mathcal{IG}\\left(\\alpha+\\frac{n}{2}, \\beta+\\frac{\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2}{2}\\right)$"
      ],
      "metadata": {
        "id": "WWQ434ZdCS2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_alpha = PRIOR_INV_GAMMA_ALPHA + observations.shape[0]//2\n",
        "posterior_beta = PRIOR_INV_GAMMA_BETA + tf.reduce_sum((observations - sample_mean)**2)//2\n",
        "poster_dist  = tfd.InverseGamma(concentration=posterior_alpha,\n",
        "                                  scale=posterior_beta)\n",
        "true_posterior_mean = tf.sqrt(poster_dist.mean())\n",
        "true_posterior_std = poster_dist.stddev()"
      ],
      "metadata": {
        "id": "HyH-mY5UCyU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.sqrt(true_posterior_mean)"
      ],
      "metadata": {
        "id": "He5VsS4WE3Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics  { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "accept_rate_mh = tf.math.count_nonzero(mh_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OZI3Be8heM24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first chain for plotting auto-correlation\n",
        "sm.graphics.tsa.plot_acf(mh_states[:, 0],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states[:, 0],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states[:, 0]\n",
        "hmc_dist = hmc_states[:, 0]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean, color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=true_posterior_mean, color='black', linestyle='-', label='true posterior mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aXbFVlupkFTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of unknown $\\sigma^2$ and $\\mu$\n",
        "\n",
        "Now the problem comes to estimate the joint distribution of $\\sigma^2$ and $\\mu$.\n",
        "\n",
        "We can define the prior distribution of $\\sigma^2$ as the inverse gamma distribution according to the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior).  That is, $\\pi(\\sigma^2) \\sim \\mathcal{IG}(\\alpha, \\beta)$. And $\\pi(\\mu) \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n",
        "\n",
        "Data likelihood function $l(x|\\mu, \\sigma^2) = \\prod_{i=1} \\mathcal{N}(x_i| \\mu, \\sigma^2)$.\n",
        "\n",
        "The posterior distribution $p(\\sigma^2, \\mu| x) ∝ l(x|\\mu, \\sigma^2)\\pi(\\sigma^2)\\pi(\\mu)$ is a Normal-inverse-Gamma distribution (Example 7.3, pp.202-203, [Introducing Monte Carlo Methods with R](https://www.amazon.com/dp/1441915753))."
      ],
      "metadata": {
        "id": "elFeW8dilIJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_state = 1.5*tf.random.uniform([N_CHAINS, 1, 2])\n",
        "\n",
        "\"\"\"\n",
        "In this case, the dimension of sigma is [1,], in order to align the shape with mu\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "define prior distribution\n",
        "\"\"\"\n",
        "def prior_dist(parameter):\n",
        "\n",
        "  return tfd.JointDistributionNamed(dict(\n",
        "                mu=tfd.Normal(loc=PRIOR_MEAN,\n",
        "                                  scale=PRIOR_STD),\n",
        "                sigma=tfd.InverseGamma(concentration=PRIOR_INV_GAMMA_ALPHA,\n",
        "                                  scale=tf.constant(PRIOR_INV_GAMMA_BETA)))).log_prob({'mu': parameter[..., 0],\n",
        "                                                                                  'sigma': parameter[..., 1]})\n",
        "\n",
        "\"\"\"\n",
        "define target distribution\n",
        "\"\"\"\n",
        "def posterior_dist(observations):\n",
        "    def _compute_posterior(parameter):\n",
        "        return tf.reduce_sum(tfd.MultivariateNormalTriL(loc=parameter[..., 0],\n",
        "                                                        scale_tril=parameter[..., 1:]).log_prob(\n",
        "            observations), axis=0)[..., tf.newaxis] \\\n",
        "            + prior_dist(parameter)\n",
        "\n",
        "    return _compute_posterior\n",
        "\n",
        "\"\"\"\n",
        "Sampling\n",
        "\"\"\"\n",
        "\n",
        "transformed_bijector = tfb.Exp()\n",
        "\n",
        "@tf.function\n",
        "def run_metropolis_hasting():\n",
        "  mh_kernel = RandomWalkMetropolis(posterior_dist(replicate_observations),\n",
        "                                    new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=0.2))\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state, # we use sigma as matrix here\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        num_steps_between_results=0,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(mh_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "  return states, kernels_results\n",
        "\n",
        "@tf.function\n",
        "def run_hmc():\n",
        "  hmc_kernel = HamiltonianMonteCarlo(posterior_dist(replicate_observations),\n",
        "                                    step_size=0.03,\n",
        "                                    num_leapfrog_steps=3)\n",
        "\n",
        "  states, kernels_results = sample_chain(num_results=num_mcmc_samples,\n",
        "                                        current_state=init_state,\n",
        "                                        num_burnin_steps=num_burn_in_steps,\n",
        "                                        kernel=tfp.mcmc.TransformedTransitionKernel(hmc_kernel,\n",
        "                                                                                    bijector=transformed_bijector),\n",
        "                                        seed=seed)\n",
        "\n",
        "  return states, kernels_results\n",
        "\n",
        "mh_states, mh_results = run_metropolis_hasting()\n",
        "hmc_states, hmc_results = run_hmc()\n",
        "\n",
        "mh_states_mean = tf.squeeze(mh_states[..., 0], axis=[-1]).numpy()\n",
        "mh_states_std = tf.squeeze(mh_states[..., 1], axis=[-1]).numpy()\n",
        "hmc_states_mean = tf.squeeze(hmc_states[..., 0], axis=[-1]).numpy()\n",
        "hmc_states_std = tf.squeeze(hmc_states[..., 1], axis=[-1]).numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "XFfYfgcB4ooc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics For Mean { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states_mean,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states_mean,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "accept_rate_mh = tf.math.count_nonzero(mh_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "accept_rate_hmc = tf.math.count_nonzero(hmc_results.inner_results.is_accepted, axis=0)/num_mcmc_samples\n",
        "\n",
        "print(f\"Acceptance rate for MH algorithm {accept_rate_mh}\")\n",
        "print(f\"Acceptance rate for HMC algorithm {accept_rate_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states_mean,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states_mean,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JasZq8y-Kgmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first chain for plotting auto-correlation\n",
        "sm.graphics.tsa.plot_acf(mh_states_mean[:, 1],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states_mean[:, 1],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states_mean[:, 1]\n",
        "hmc_dist = hmc_states_mean[:, 1]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_mean, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcV03xNx7zvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diagonics For Variance { display-mode: \"form\" }\n",
        "\n",
        "r_hat_mh = tfp.mcmc.potential_scale_reduction(\n",
        "    mh_states_std,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "r_hat_hmc = tfp.mcmc.potential_scale_reduction(\n",
        "    hmc_states_std,\n",
        "    independent_chain_ndims=1,\n",
        "    split_chains=True)\n",
        "\n",
        "print(f\"R^hat for MH algorithm {r_hat_mh}\")\n",
        "print(f\"R^hat for HMC algorithm {r_hat_hmc}\")\n",
        "\n",
        "ess_mh = tfp.mcmc.effective_sample_size(\n",
        "    mh_states_std,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "ess_hmc = tfp.mcmc.effective_sample_size(\n",
        "    hmc_states_std,\n",
        "    cross_chain_dims=None,\n",
        ")\n",
        "\n",
        "print(f\"Effective sample size for MH algorithm {ess_mh}\")\n",
        "print(f\"Effective sample size for HMC algorithm {ess_hmc}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-wJ9k_087stM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first chain for plotting auto-correlation\n",
        "sm.graphics.tsa.plot_acf(mh_states_std[:, 2],\n",
        "                         title='Autocorrelation of MH algorithm',\n",
        "                         lags=60)\n",
        "sm.graphics.tsa.plot_acf(hmc_states_std[:, 2],\n",
        "                         title='Autocorrelation of HMC algorithm',\n",
        "                         lags=60)\n",
        "\n",
        "# plot the fist chain\n",
        "\n",
        "mh_dist = mh_states_std[:, 2]\n",
        "hmc_dist = hmc_states_std[:, 2]\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(mh_dist, ax=ax, label='Histogram of MH algorithm')\n",
        "# lines = ax.hist(mh_dist, bins=100, edgecolor=\"k\", label=\"Histogram of MH algorithm\")\n",
        "ci_mh = credible_interval(mh_dist)\n",
        "ax.axvline(x=ci_mh[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_mh[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(mh_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter plot of data samples and histogram\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.distplot(hmc_dist, ax=ax, label='Histogram of HMC algorithm')\n",
        "# lines = ax.hist(hmc_dist, bins=100, edgecolor=\"k\", label=\"Histogram of HMC algorithm\")\n",
        "ci_hmc = credible_interval(hmc_dist)\n",
        "ax.axvline(x=ci_hmc[0], color='blue', linestyle='--', label='2.5% CI')\n",
        "ax.axvline(x=ci_hmc[1], color='blue', linestyle='--', label='97.5% CI')\n",
        "ax.axvline(x=true_std, color='red', linestyle='-', label='true mean')\n",
        "ax.axvline(x=np.mean(hmc_dist), color='red', linestyle='--', label='estimated posterior mean')\n",
        "\n",
        "ax.legend(loc=\"best\")\n",
        "ax.grid(True, zorder=-5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7u2TpoWg9D4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}